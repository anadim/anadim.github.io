% -----------------------------*- LaTeX -*------------------------------
\documentclass[12pt]{report}
\usepackage{scribe}
\usepackage{amsfonts,amsmath,amsbsy,color,hyperref,amssymb,graphicx,listings,empheq,subcaption,listings,color,float}
\begin{document}
\scribe{Ananth Sridhar, Ashwin Varadarajan}
\lecturenumber{3}			% required, must be a number
\lecturedate{09/13}		% required, omit year

\def \Expec{\mathbb E}
\def \Prob{\mathbb P}
\def \Risk{\mathfrak R}
\def \NormD{\mathcal N}
\def \HypClass{\mathcal H}
\def \TODO{\textbf{\textcolor{red}{TODO: }}}

\maketitle

\framebox[.95\textwidth]{\parbox{.93\textwidth}{ {{\bf Note:}} These
lecture notes are still rough, and have only have been mildly proofread.  }}
\vspace*{.1in}
% ----------------------------------------------------------------------
\section{Review}

\begin{itemize}
\item The Empirical Risk concentrates for \textbf{finite hypothesis classes}
\begin{itemize}
\item \textit{Floating Point Assumption:}
\begin{equation}
n_{\text{samples}} = O\left(\frac{n_{\text{parameters}}}{\epsilon^2}\right)
\end{equation}
\end{itemize}
\item Empirical risk and true risk (concentration). If, $h$ is a predictor,
\begin{equation}
\hat{\Risk}[h] \le \Risk[h] + \epsilon
\end{equation}
\item Earlier, we assumed that the cardinality of the hypothesis class H is finite. Can we handle infinite hypothesis classes? Yes, using the VC-dimension (section \ref{vc_dimension}).
\end{itemize}

\section{VC dimension} \label{vc_dimension}

\begin{itemize}
\item From Vapnik-Chervonenkis Theory [1971]
\item The VC-dimension expresses the predictive power of a classifier
\item In the general case, finding the VC-dimension of a Hypothesis class is an NP-Hard problem. However, there are a few special cases of Hypothesis classes where the VC-dimension is easy to calculate (see example). Also, we can always bound the VC-dimension if the Hypothesis class is a finite set, as in (eqn \ref{vc_dimension_bound}).
\end{itemize}

If $\HypClass$ is the hypothesis class,

\begin{equation} \label{vc_dimension_bound}
\text{VC}_\text{dimension}[\HypClass] \leq \log |\HypClass|
\end{equation}

The above inequality implies that the VC dimension is a stronger notion of complexity than simply the number of predictors.

\paragraph{Example:} if $h$ is a predictor, and $\HypClass$ is the hypothesis class

\begin{gather*}
\text{If,} \quad h(x_i) = y_i \quad \forall \quad i \in \{1 \dots n\}, \quad \text{for any $n$} \\
\text{VC}_\text{dimension}[\HypClass] = \infty
\end{gather*}

\begin{gather*}
\text{If,} \quad \HypClass = \{ \text{All Hyperplanes in $d$ dimensions} \} \\
\text{VC}_\text{dimension}[\HypClass] = d + 1
\end{gather*}

\section{Concentration of Empirical Risk}

The following holds with probability $1 - \delta$, for finite and infinite hypothesis classes.

\begin{equation}
\underset{h \in \HypClass}{\sup} \left| \hat{\Risk}_n[h] - \Risk[h] \right| \le O\left( \sqrt{\frac{\text{VC}_\text{dimension}[\HypClass] \log\left(\frac{n}{\text{VC}_\text{dimension}[\HypClass]}\right) + \log \frac{1}{\delta} }{ n }} \right)
\end{equation}

\subsection{Concentration of Empirical Risk for ERM}

\begin{align}
\hat{h}^* & = \underset{h \in \HypClass}{\text{arg min}}\quad \hat{\Risk}_n[h] & \text{(Empirical Risk Minimizer)} \\
h^* & = \underset{h \in \HypClass}{\text{arg min}}\quad \Risk[h] & \text{(True Risk Minimizer)} \\
\Risk[\hat{h}^*] - \Risk[h^*] & = \quad ? & \text{(How close are we?)}
\end{align}

\begin{theorem}
The empirical risk of the empirical risk minimizer concentrates around the true risk of the true risk minimizer
\end{theorem}

\begin{proof}
\begin{align}
\Risk[\hat{h}^*] & = \left\{
  \begin{aligned} 
     \Risk[\hat{h}^*] & + \left( \Risk[h^*] - \Risk[h^*] \right) \\
    & + \left( \hat{\Risk}_n[h^*] - \hat{\Risk}_n[h^*] \right) \\
    & + \left( \hat{\Risk}_n[\hat{h}^*] - \hat{\Risk}_n[\hat{h}^*] \right)
  \end{aligned} \right. \\
\Risk[\hat{h}^*] - \Risk[h^*] & = \left\{
  \begin{aligned} 
      & \left( \hat{\Risk}_n[h^*] - \Risk[h^*] \right) \quad & \text{\textcolor{red}{concentrates: $\le \epsilon$}} \\
    + & \left( \Risk[\hat{h}^*] - \hat{\Risk}_n[\hat{h}^*] \right) \quad & \text{\textcolor{red}{concentrates: $\le \epsilon$}} \\
    + & \left( \hat{\Risk}_n[\hat{h}^*] - \hat{\Risk}_n[h^*] \right) \quad & \text{\textcolor{red}{concentrates: $\le 0$}}
  \end{aligned} \right. \\
\Risk[\hat{h}^*] - \Risk[h^*] & \le 2 \epsilon
\end{align}
\end{proof}

\section{Examples of learning problems}
The following are some typical classes of problems in Machine Learning:

\subsection{Linear Regression}
Fitting a line or hyperplane to data points so as to minimize the sum of distance of predicted labels to the observed ones. Involves minimizing the norm:  

\begin{equation}
\text{min} \quad \frac{1}{n} \sum_{i=1}^n ( x_i^Tw - y_i )^2 
\end{equation}

\subsection{Binary Classification}
Assigning binary labels to data, hence the variable \textit{y} is discrete and \textit{y}$|$\textit{x} is a Bernoulli distribution. Involves solving the logistic regression:

\begin{equation}
\text{min} \quad \frac{1}{n} \sum_{i=1}^n \log ( 1 + \exp(-y_ix_i^Tw))
\end{equation}

\subsection{Support Vector Machines}
Involves minimizing the maximum margin from a classifier, with a regularization term.

\begin{equation}
\text{min} \quad \frac{1}{n} \sum_{i=1}^n \max(0,1 - y_i x_i^T w) + \lambda \|w\|
\end{equation}

\subsection{Feed Forward Neural Network}
Each layer involves a linear sum of weighted features, passed through an activation function $ \sigma $, which is usually a step function or a smooth version of it, such as $ \sigma (x) = -1/(1+exp(-x))$. This can be repeated over a number of hidden layers between the input and the output layers (deep networks).  

\section{Useful properties for loss functions}
In general, the problem $\underset{x}{\text{min}} f(x)$ is hard to solve as is, and hence we use additional properties of the loss function \textit{f} to speed up computation.
\subsection{Convex Functions}

Any local minimum of the function is also a global minimum.

\begin{align}
\begin{aligned}
f( \alpha \vec{x} + (1-\alpha) \vec{y} ) & \le \alpha f(\vec{x}) + (1-\alpha) f(\vec{y}) \\
& \forall \alpha \in [0,1], \quad \vec{x}, \vec{y}
\end{aligned}
\end{align}

\subsection{L-Lipschitz functions}

The function does not change too fast.

\begin{equation}
|f(\vec{x}) - f(\vec{y})| \le L || \vec{x} - \vec{y} ||
\end{equation}

\subsection{$\beta$-smooth functions}

The function has a Lipschitz gradient.

\begin{equation}
|| \nabla f(\vec{x})  - \nabla f(\vec{y}) || \le \beta || \vec{x} - \vec{y} ||
\end{equation}

\subsection{$\lambda$-strongly convex functions}

The function has a unique global minimum.

\begin{align}
\begin{aligned}
f( a \vec{x} + (1-a) \vec{y} ) & \le a f(\vec{x}) + (1-a) f(\vec{y}) - \frac{\lambda}{2} a(1-a) ||\vec{x} - \vec{y}||^2  \\
& \forall a \in [0,1], \quad \vec{x}, \vec{y}
\end{aligned}
\end{align}

In other words, $f$ is $\lambda$-strongly convex \textbf{iff}

\begin{equation}
f(\vec{x}) - \lambda || \vec{x} ||^2 \quad \text{is convex}
\end{equation}

This property is especially nice, since we can achieve this for ERM by regularization. Strong convexity gives improved algorithmic convergence rates (the algorithms make use of the regularization for faster convergence). Note: All convex functions are 0-strongly convex (trivial case).

\section{Additional Reading}

\begin{itemize}
\item A nice reference on the VC-dimension (explained with an example): \href{https://www.cs.princeton.edu/courses/archive/spring13/cos511/scribe_notes/0221.pdf}{Link}
\item An additional resource for insights into strong convexity and some settings where it is utilized: 
\href{https://blogs.princeton.edu/imabandit/2013/04/04/orf523-strong-convexity/}{Link}
\end{itemize}

\end{document}