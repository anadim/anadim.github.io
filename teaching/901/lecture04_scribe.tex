% -----------------------------*- LaTeX -*------------------------------
\documentclass[12pt]{report}
\usepackage{scribe}
\usepackage{amsmath, bm}
\usepackage{graphicx,epstopdf}
\def\cD{\mathcal D}
\def\cH{\mathcal H}
\def\bx{\bm{x}}
\def\by{\bm{y}}
\def\bw{\bm{w}}
\def\bu{\bm{u}}
\def\bbE{\mathbb E}
\def\bbR{\mathbb R}
\begin{document}
\scribe{Muxuan Liang}
\lecturenumber{4}			% required, must be a number
\lecturedate{09/15}		% required, omit year

\maketitle

%\framebox[.95\textwidth]{\parbox{.93\textwidth}{ {{\bf Note:}} These
%lecture notes are still rough, and have only have been mildly proofread.  }}
%\vspace*{.1in}


% ----------------------------------------------------------------------


%\begin{danger}
%This is the danger environment.
%\end{danger}

\section{Properties of Functions}

\subsection{Some useful properties of functions}
Suppose that $f:\bf X\to \bbR$, $\nabla f(\bf x)$  exists and is a column-based gradient vector. We will discuss about the following properties of a function

\begin{itemize}
 \item $L$-Lipschitz: We say a function is $L$-Lipschitz if $|f(\bx)-f(\by)|\leq L \|\bx-\by \|_2$.
 \item $\beta$-smooth: We say a function is $\beta$-smooth if $\|\nabla f(\bx)-\nabla f(\by)\|\leq \beta\|\bx-\by\|_2$.
 \item $\lambda$-strong convex: We say a function is $\lambda$- strong convex if $f(\bx)\geq f(\by)+<\nabla f(\by), \bx-\by>+\frac{\lambda}{2}\|\bx-\by\|_2^2$. This implies that strong convex function is always bounded by a quadratic function from below.
\end{itemize}


\subsection{Examples}

Table \ref{tab:1} is a summary for some common functions and their properties regarding as convex, $L$-Lipschitz, and $\beta$-smooth.

\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
  & Convexity & $L$-Lipschitz & $\beta$-smooth \\ 
 \hline
 $|x|$& Convex & 1 &  \\  
 $x^2$& Convex & $\max_{x\in D}  2x$ & 2\\
 $log(1+e^x)$& Convex & 1 & 1/4\\
 $\bw^\top\bx+b$ & Convex & $\|\bw\|_2$ & 0\\
 $g(\bw^\top\bx+b)$ & if $g$ convex & $\|\bw\|_2L_g$, if $g$ $L_g$-Lipschitz & $\|\bw\|_2^2\beta_g$, if $g$ $\beta_g$-smooth\\
 \hline
\end{tabular}
\label{tab:1}
\end{center}

\subsection{Some useful results}

Suppose that we have a family of convex function $\mathcal{F}$, then $\sup_{f\in \mathcal{F}} f(x)$ is also a convex function. If a function $f$ is $L$-Lipschitz, then $\|\nabla f(\bx)\|_2\leq L$. If a function $f$ is $\beta$-smooth, then $\|\nabla f(\bx)\|_2^2\leq \beta f(\bx)$. If a function $f$ is $\lambda$-strong convex, then $f(\bx)-\frac{\lambda}{2}\|\bx\|_2^2$ is also convex.

\section{Importance of convexity}

Suppose $f(\bx)$ is convex and differentiable, for any point $\bx_0$, the linearization of $f(\bx)$ is a minorization at point $\bx_0$. To be specific,
\begin{equation}
f(\bx)\geq f(\bx_0)+<\nabla f(\bx_0), \bx-\bx_0>,
\end{equation} 
which implies that $\nabla f(\bx^*)=0$, where $f(\bx^*)=\min f(\bx)$. Figure (\ref{Fig:convex}) show the relationship of the linearization and the minimizer.

\begin{figure}
\includegraphics[scale=0.5]{convex}
\label{Fig:convex}
\caption{The figure show the linearization of a quadratic function. The dashed lines are linearization and solid curve is the quadratic function. The horizontal dashed line implies the gradient of the minimizer is 0. }
\end{figure}








\section{Gradient Descent}

Our goal is to get $\min_{\bx} f(\bx)$ and minimizer $\bx^*$. Suppose that we have a guess or last iteration of the minimizer $\bx_k$, we can write our next guess or iteration $\bx_{k+1}=\bx_k + \bu_k$. We hope that when $k\to +\infty$, $\|\nabla f(\bx_k)\|\to 0$. Our idea is to get $\bx_{k+1}$ through the following optimization
\begin{equation}\label{eq:GDlinear}
\bx_{k+1} = arg\min_{\bx} \{f(\bx_k)+<\nabla f(\bx), \bx-\bx_k>+\frac{1}{2\gamma}\|\bx-\bx_k\|_2^2\}.
\end{equation}

We denote the right-hand size of equation (\ref{eq:GDlinear}) as $g(\bx)$. The first two term is the linearization of $f(\bx)$ at $\bx_k$, which gives the descent direction. The last term is the regularization to keep $x$ close to $\bx_k$. Note that equation (\ref{eq:GDlinear}) is a quadratic programming without constraints. We can take the derivative and set it to be 0. Then we can get
\begin{equation}\label{eq:GDupdate}
\bx_{k+1}=\bx_k-\gamma\nabla f(\bx_k).
\end{equation}

\subsection{Convergence of Gradient Descent}

We will investigate the convergence result of the Gradient Descent method (\ref{eq:GDupdate}).
\begin{theorem}
\label{ThmGD}
If we set $\gamma=\frac{\Delta_1}{L\sqrt{T}}$, where $L$ is the Lipschitz constant of $f(\bx)$ and $\Delta_k=\|\bx_k-\bx^*\|_2$, then
\begin{equation}
f(\frac{1}{T}\sum_{k=1}^T \bx_k))-f(\bx^*)\leq \frac{\Delta_1 L}{\sqrt{T}}.
\end{equation}
\end{theorem}

\begin{proof}
We start from the convexity of $f$. We know that
\begin{align*}
f(\bx_k)-f(x^*)&\leq <\nabla f(\bx_k), \bx_k-\bx^*>\\
& =<\frac{1}{\gamma}(\bx_{k+1}-\bx_k), \bx_k-\bx^*>\\
& =\frac{1}{2\gamma}\left( \Delta_k^2-\Delta_{k+1}^2+\gamma^2\|\nabla f(\bx_k)\|_2^2  \right).
\end{align*}

For $f$ is $L$-Lipschitz, $\|\nabla f(\bx_k)\|_2^2\leq L^2$, then we have the following inequality
\begin{equation*}
f(\bx_k)-f(x^*)\leq \frac{1}{2\gamma}(\Delta_k^2-\Delta_{k+1}^2)+\frac{\gamma}{2}L^2.
\end{equation*}
We take $k=1,\dots,T$ and add those inequality up. Then we can get
\begin{equation*}
\sum_{k=1}^T f(\bx_k)-f(x^*)\leq \frac{1}{2\gamma}\Delta_1^2+\frac{\gamma}{2}TL^2.
\end{equation*}
By convexity and optimize over $\gamma$, when we set $\gamma=\frac{\Delta_1}{L\sqrt{T}}$, we have
\begin{equation*}
f(\frac{1}{T}\sum_{k=1}^T \bx_k))-f(\bx^*)\leq \frac{\Delta_1 L}{\sqrt{T}}.
\end{equation*}
\end{proof}

\end{document}

