% -----------------------------*- LaTeX -*------------------------------
\documentclass[12pt]{report}
\usepackage{scribe}
\usepackage{amsmath,scribe,hyperref,graphicx,color,algorithm}
\usepackage{algpseudocode}
\begin{document}
\scribe{Lingjiao Chen}
\lecturenumber{10}			% required, must be a number
\lecturedate{06/10}		% required, omit year

\maketitle

\framebox[.95\textwidth]{\parbox{.93\textwidth}{ {{\bf Note:}} These
lecture notes are still rough, and have only have been mildly proofread.  }}
\vspace*{.1in}


% ----------------------------------------------------------------------

\section{Recap of Projection-based Constraint Optimization}
In the previous lecture, we discussed how to solve the convex constraint optimization problem
\begin{equation}
\begin{split}
& \min f(x)\\
& s.t. x\in C
\end{split}
\end{equation}
where $C$ is a convex closed set. A widely used method is projected gradient descent (PGD), the key idea of which is to apply projection after gradient descent. More precisely, PGD uses
\begin{equation}
\begin{split}
 X_{k+1} = P_{C}(X_k - \gamma_k g_k)
\end{split}
\end{equation}
at iteration $k$, where $\gamma_k$ is the step size at iteration $k$, $g_k$ is the (approximate) gradient at iteration $k$, and $P_C(x) = \min_{y\in C}{||x-y||_2^2} $ is the projection operator. As we have seen from the last lecture, PGD has the advantage that it achieves similar convergence rate as the traditional gradient descent method for unconstrained optimization problem. 

However, the computational cost of projection can be prohibitively large. For example, consider the feasible set $C_{PSD(s)} = \{ X \in R^n\times n| X \textrm{ is }  psd, Tr(X) \leq s \}$. The computational cost of projection on $C_{PSD(s)}$ in general is $O(n^3)$, which is unacceptable when $n$ is very large. In such scenario, one would seek to have faster algorithms.   
\section{Projection-free Gradient Descent}
Note that the key bottleneck of PGD is the projection operator. Thus, a natural idea is to accelerate the algorithm by avoiding projection. Frank-Walfe method utilizes exactly this idea by, roughly speaking, replacing the quadratic term by a linear one in the objective function. The precise algorithm is shown in Algorithm \ref{FWA}.
\begin{algorithm}
	\caption{Frank-Walfe Algorithm}\label{FWA}
	\begin{algorithmic}
		\State $X_0 \gets \textrm{ initial value }$
		\For{ $ k = 0:T $}
			\State $g_k^{FULL} = \nabla f(X_k)$
			\State $ g_k = \arg\min_{y\in C}<y,g_k^{FULL} > $ 
			\State $ X_{k+1} = (1-\gamma_k)X_k + \gamma g_k$ 
		\EndFor
	\end{algorithmic}
\end{algorithm}

\subsection{An demonstrative example}
Considering that the key component in Frank-Walfe Algorithm is $ g_k = \arg\min_{y\in C}<y,g_k^{FULL} > $, we are interested in obtaining an intuitive understanding of it. Consider $C = \{ X | ||X||_2 \leq L \}$. Then we have 
$ g_k = \arg\min_{y\in C}<y,g_k^{FULL} >  = - L \frac{\nabla f(x)}{|| \nabla f(x)||_2 }$. In other words, Frank-Walfe reduces to the classic gradient descent method.

\subsection{Convergence Analysis}
The next question is if/how Frank-Walfe Algorithm converges. The following theorem gives the answer.\\
\begin{theorem}
	\label{FWA_Converge}
	If $f$ is $\beta-$smooth, $\gamma_k = \frac{1}{K+1}$, then
\begin{equation}
	f(X_T) - \min_{x\in C}f(x) \leq O(1) \frac{\beta R^2}{T+1},
\end{equation}	
where $R = \sup_{x,y \in C} ||x-y ||_2$.	
\end{theorem}

\begin{proof}
For simplicity, let $f^*$ and $X^*$ denote the optimal function value and the optimal solution.
From $\beta-$smooth, we have 
\begin{equation}
	f(X_{k+1}) - f(X_{k}) \leq  < \nabla f(X_k), X_{k+1} - X_{k}> +\frac{\beta}{2} || X_{k+1} - X_{k}||^2.
\end{equation}
Noting that $X_{k+1} = (1-\gamma_k) X_k +\gamma_k g_k$, we have
\begin{equation}
\begin{split}
f(X_{k+1}) - f(X_{k}) &\leq \gamma_k < \nabla f(X_k), g_k - X_{k}> +\frac{\beta \gamma_k^2 }{2} || g_{k} - X_{k}||^2\\
&\leq  \gamma_k < \nabla f(X_k), X^* - X_k > +\frac{\beta \gamma_k^2 R^2 }{2}\\
&\leq   \gamma_k (f^* - f(X_k)) +\frac{\beta \gamma_k^2 R^2 }{2},\\
\end{split}
\end{equation}
which implies 
\begin{equation}
\begin{split}
f(X_{k+1}) - f^* &\leq (1-\gamma_k)f(X_{k}-f^*) + \frac{\beta}{2} \gamma_k^2 R^2.\\
\end{split}
\end{equation}
Let $\gamma_k = \frac{1}{k+1}$. By induction, one can easily see that 
\begin{equation}
f(X_{k+1}) - f^* \leq O(1) \frac{\beta R^2}{k+1}.
\end{equation}

\end{proof}

\subsection{Revisiting an Example}
How much computational cost does Frank-Wolfe Algorithm save compared to PGD? Although it is hard to answer in general, we give an example to have a taste. Still consider the
convex set $C_{PSD(s)}$. Note that $X$ is symmetric psd, so $\exists V$ such that $X = V^T V$. Thus,
\begin{equation}
\begin{split}
&\min_{X\in C_{PSD(s)} } <X,Y>\\ 
=&\min_{ || V|| < \sqrt{s} } Tr<V^T Y V>\\ 
=&\min_{ || V|| < \sqrt{s} } Tr<V^T Y V>\\ 
=&\min_{ || V|| < \sqrt{s} } \sum_{i=1}^{n} V_i^T Y V_i.\\ 
\end{split}
\end{equation}
The problem is equivalent to find the maximum eigenvalue of $Y$, which requires only $O(n)$ computations. In other words, for $C_{PSD(s)}$, Frank-Wolfe Algorithm can save $O(n)$ computational cost compared to PGD.

\subsection{Open Questions}
One major disadvantages of Frank-Wolfe Algorithm is it requires computing the full gradient at each iteration. It is not acceptable in machine learning application when there are a large amount of data and it is expensive to compute the full gradient. One may take it as granted to develop a stochastic Frank-Wolfe Algorithm (SFWA) by replacing the full gradient with a sampled gradient, but it turns out to be difficult. So far, there is no known SFWA with one sample gradient computation per iteration and fast convergence guarantee. A related work is \textit{Projection-free Online Learning} coauthored by Elad Hazan and Satyen Kale, where they developed SFWA for online learning. We refer interested readers to this paper and its follow-ups for more information.




\end{document}

