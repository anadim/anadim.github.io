% -----------------------------*- LaTeX -*------------------------------
\documentclass[12pt]{report}
\usepackage{amsmath,scribe,hyperref,graphicx,color,algorithm}
\usepackage{algpseudocode}
\usepackage[titletoc,title]{appendix}

\def \E{\mathbb E}
\def \P{\mathbb P}

\begin{document}
\scribe{Alisha Zachariah}
\lecturenumber{7}			% required, must be a number
\lecturedate{09/27}		% required, omit year

\maketitle

%\framebox[.95\textwidth]{\parbox{.93\textwidth}{ {{\bf Note:}} These
%lecture notes are still rough, and have only have been mildly proofread.  }}
%\vspace*{.1in}


% ----------------------------------------------------------------------

%\begin{danger}
%This is the danger environment.
%\end{danger}

\section{Re-examining Stochastic Gradient Descent}

The motivating idea behind \textit{Stochastic Gradient Descent} (SGD) is ensuring cheap per iteration complexity. 

Recall the set-up:
\begin{align*} \min \;& \frac{1}{n} \sum_{i=1}^n f_i(x) \\
\text{Update:  } x_{k+1} &= x_k - \gamma \nabla f_{s_k}(x_k) 
\end{align*}

While the cost per iteration of SGD is $\approx n$ times faster than \textit{Gradient Descent} (GD), the iteration complexity of SGD is significantly worse (exponentially worse, see \href{http://papail.io/teaching/901/lecture06_scribe.pdf}{6.1.4}).

The goal of faster iteration complexity while maintaining efficiency per iteration motivates \textit{Stochastic Variance Reducing Gradient} (SVRG). The derivation of convergence bounds for SGD motivates the design of the SVRG algorithm.



%\begin{theorem}
%\label{ThmNeat}
%This is a neat theorem.
%\end{theorem}
%
%\begin{proof}
%Lengthy and technical proof.
%\begin{equation}
%\sum_{i=1}^n x_i = y.
%\end{equation}
%\end{proof}


\subsection{Convergence bounds for SGD:}

The convergence rates we derived previously suggest a worst case convergence rate of $\sim 1/T$ for SGD, where $T$ is the  number of iterations. In practice, the actual convergence rate may be somewhat better than this bound. Infact, we should expect SGD to begin with linear convergence comparable to GD, eventually slowing down to the $1/T$ rate predicted by our analysis (Fig. 7.1). We will develop the intution for why to expect this. SVRG is intended to address this slowing down.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.3]{scribepic.jpg}
\caption{Convergence rates: Worst-case bound in blue vs. true rate in red. }
\end{figure}

We will make the following assumptions regarding he objective function $f$: 
\begin{itemize}
\item $\lambda$-strong convexity: $\langle \nabla f(x) -\nabla f(y), x-y\rangle \ge \lambda \|x -y\|^2$
\item $\beta$-smoothness: $\| \nabla f(x) - \nabla f(y) \|^2 \le \beta  \|x -y\|^2$
\item $E_s \| \nabla f_s (x)\| \le M^2$ 
\end{itemize}

%Here is how to typeset an array of equations.
%
%\begin{eqnarray}
%	x & = & y + z \\
%%
%     \alpha & = & \frac{\beta}{\gamma}
%\end{eqnarray}

\textit{Notation} : $\Delta_k = \| x_k - x^*\|^2$

Recall that using $\lambda$-strong convexity we get 
\begin{align*}
 \E \Delta_{k+1}  \le \underbrace{(1- \gamma \lambda) \E \Delta _k}_{\text{const. factor decrease}} + \underbrace{\gamma^2 \E \| \nabla f_{s_k} (x_k)\|^2 }_{\text{variance of gradient, causes slowing}}
\end{align*}

Also recall, at this point in the analysis of GD we use $\beta$-smoothness and that $\nabla f(x^*) =0$ to get constant factor decrease:
\begin{align*}
  \Delta_{k+1}  & \le (1- \gamma \lambda) \Delta _k + \gamma^2  \| \nabla f (x_k)\|^2 \\
&= (1- \gamma \lambda) \Delta _k + \gamma^2  \| \nabla f (x_k)- \nabla f(x^*)\|^2 \;\;\;(\nabla f(x^*) = 0)\\
& \le (1- \gamma \lambda)  \Delta _k + \gamma^2 \beta^2 \Delta_k  \;\;\;(\beta\text{-smoothness})\\
&= (1- \gamma \lambda + \gamma^2 \beta^2 ) \Delta_k
\end{align*}

Replicating this idea for SGD:
\begin{align*}
\E \Delta_{k+1} & \le  (1- \gamma \lambda)\E \Delta _k +\gamma^2 \E \| \nabla f_{s_k} (x_k)\|^2 \\
 & =  (1- \gamma \lambda)\E \Delta _k +\gamma^2 \E \| \nabla f_{s_k} (x_k) - \nabla f_{s_k}(x^*) +\nabla f_{s_k}(x^*) \|^2 \\
& = (1- \gamma \lambda)\E \Delta _k +2\gamma^2 \E \| \nabla f_{s_k} (x_k) - \nabla f_{s_k}(x^*) \|^2 +2\gamma^2 \E \|\nabla f_{s_k}(x^*)\|^2 \text{ (by \ref{PL})}
\end{align*}

where the $\beta$-smooth assumption 
\begin{equation}
\E \Delta_{k+1}  \le \underbrace{(1- \gamma \lambda+2\beta^2 \gamma^2)\E \Delta _k}_{A} + \underbrace{2\gamma^2 \E \|\nabla f_{s_k}(x^*)\|^2}_{B}
\end{equation}

Initially $B<<A$ and we observe the linear rate regime, once $B>A$ we observe $1/T$-rate.

%\subsection{Yet another subsection}


%\begin{corollary}
%\label{CorThmNeat}
%A corollary of Theorem~\ref{ThmNeat}.
%\qed
%\end{corollary}

\section{Stochastic Variance Reducing Gradient}

The idea behind \textit{Stochastic Variance Reducing Gradient} (SVRG) is to ensure decaying "variance" and SGD-like cost/iteration.
\begin{align*}
\text{Update: }v_k \\
x_{k+1} = x_k - \gamma v_k
\end{align*}
We would like to pick $v_k$ such that $\E v_k = \nabla f$, ie. we would like unbiased "gradients".

$$ v_k = \nabla f_{s_k}(x_k) \textcolor{red}{ - \nabla f_{s_k}(y)\;\;+} \underbrace{\textcolor{red}{\nabla f(y)}}_{\text{ensures unbiased}}$$

The terms in red should be variance reducing. For now $y$ is fixed to minimize the number of full gradient computations and needs to be chosen.

\subsection{Analysis of SVRG}
\vspace{.1 in}
Let's analyze the "variance" :
\begin{align*}
\E_s \|v_k \|^2 &= \E \|\nabla f_s(x) - \nabla f_s(y) + \nabla f(y) \|^2 \\
& = \E \| \nabla f_s(x) - \nabla f_s(y) + \nabla f(y) \pm \nabla f_s(x^*) \|^2 \\ 
& \le 2 \E \| \nabla f_s(x) - \nabla f_s(x^*) \|^2 + 2 \E \|\nabla f_s(y) -  \nabla f_s(x^*) - \nabla f(y)\|^2  \text{  (by \ref{PL})} \\ 
& \le 2\beta^2 \E \Delta_k + 2 \E \| \nabla f_s(y) - f_s(x^*)\|^2  \text{  (by \ref{wut})}\\
& \le 2 \beta^2 \E \Delta_k + 2 \beta^2 \E  \| y - x^*\|^2 \;\;\;(\beta- \text{smoothness)}
\end{align*}

We have effectively introduced  $y$ into the bound for variance. There are now two competing forces at play: picking a fresh $y$ more often should decrease the variance, however doing this too often involves computing too many full gradients. Let's set $y =x_1$ and see what happens...

Substituting back into $7.1$:
\begin{align*}
\E \Delta_{k+1} &\le \E \Delta _k - \gamma \lambda \E \Delta_k + 2\gamma^2 \beta^2 \E \Delta_k + 2 \beta^2 \E \Delta_1  \\
& = (1 - \gamma \lambda +2\gamma^2 \beta^2)\E \Delta_k  + 2 \gamma^2\beta^2 \E \Delta_1
\end{align*}

Unrolling this: 
\begin{align*}
\E \Delta_{k+1} & \le (1 - \gamma \lambda +2\gamma^2 \beta^2)\E \Delta_k  + 2 \gamma^2\beta^2 \E \Delta_1 \\
& \le (1 - \gamma \lambda +2\gamma^2 \beta^2)^2 \E\Delta_{k-1} + \underbrace{(1 - \gamma \lambda +2\gamma^2 \beta^2)}_{*}2 \gamma^2\beta^2\E \Delta_1  + 2\gamma^2 \beta^2 \E \Delta_1\\
& \le (1 - \gamma \lambda +2\gamma^2 \beta^2)^2 \E\Delta_{k-1} + 2\gamma^2 \beta^2\E \Delta_1 + 2\gamma^2 \beta^2\E \Delta_1 \;\;\; (*\text{ must be} < 1) \\
& \vdots \\
&  \le (1 - \gamma \lambda +2\gamma^2 \beta^2)^k \E\Delta_{1} + 2k\gamma^2 \beta^2 \E \Delta_1
\end{align*}

Suppose we would like this to be $\le 0.5 \E \Delta_1$ after $T$ iterations of SVRG, what is the best choice of $T$ and $\gamma$?

If we pick $\gamma = O(1) \lambda/\beta^2$, then it turns out that we can set $T = O(1) \beta^2/\lambda^2$. This can be improved to $T = O(1) \beta/\lambda$. Note that $\beta/\lambda$ is the condition number $\kappa$. 

This is one \textbf{\textit{epoch}} of the algorithm. If $E$ is the total number of epochs executed we have
$$\E \Delta_k \le (0.5)^E \ . \ \E \Delta_1$$

\subsection{Algorithmic complexity}

\begin{algorithm}[H]
\caption{SVRG}
\begin{algorithmic}
\State $y \gets x_0 $
\State $k \gets 0$ 
\For{e = 1:E}
	\State $g \gets \nabla f(y)\;\;\;\; \textit{(full gradient)}$
	\For{s = 1:S}
  		\State $x_k \gets x_k - \gamma (\nabla f_s(x_k) - \nabla f_s(y) +g)\;\;\;\; \textit{(smaller gradient eval)}$ 
		\State $k \gets k+1$
	\EndFor
	\State $y \gets x_{k-1}$
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{itemize}
\item \textbf{SVRG:} $E \sim \log (\frac{1}{\epsilon})$ so the complexity is $O((n+\kappa) \log (\frac{1}{\epsilon}))$ 
\item \textbf{GD:}  $T \sim \kappa \log (\frac{1}{\epsilon})$ so the complexity is $O(n\kappa \log (\frac{1}{\epsilon}))$ 
\item \textbf{SGD:} $T \sim \frac{\kappa}{\epsilon}$  and the complexity is $O(\frac{\kappa}{\epsilon})$
\end{itemize}

The moral here is that even though we are allowing ourselves a few gradient computations here, we don't really pay too much in terms of complexity. 
 
The big issue in SVRG is the extra parameter, namely the number of epochs. Another issue it parallelizability -- SVRG cannot be parametrized so easily since the updates contain some history.

\subsection{Concentration}

Our analysis demonstrates that we can make $\E \Delta_k$ small. We can convert this into a statement about the probability that $\Delta_k$ is small using Markov's inequality:
 $$\text{Markov's inequality:  If }X\ge 0, \;\; \P(X \ge a\E(X)) \le \frac{1}{a} $$
$$\text{So }\;\;\P(\Delta_k \ge 10\E\Delta_k)  \le \frac{1}{10} $$

So if we run SVRG $r$ times, then the probability that $\Delta_k$ is large in all $r$ trials is less that $10^{-r}$ and we can get concentration in this way.

However, can we get exponential concentration, ie. $\P(\Delta_k \ge a\E\Delta_k) \le e^{-a}$? It may be possible to achieve this by proving some version of Hoeffding's inequality for strongly convex loss functions.

\section{Conclusion}

There are several versions of SVRG (SAG/SAGA, SDCA, Finito) with variations and applications to different kinds of problems (e.g. faster convergence to saddle points in non-convex cases). There is a lot of interest among the NIPS/ICML communities and lots of active research on how to improve SVRG.


\section{Appendix:}
\begin{itemize} 
\item \begin{equation} \label{PL}
\| a+b\|^2 \le 2 \|a\|^2 + 2 \|b\|^2
\end{equation}

This follows from the Parallelogram Law $\|a+b\|^2 + \|a-b\|^2 = 2\|a \|^2 + 2\| b\|^2$, which is easily verified.
 \item \begin{equation} \label{wut}
\E \|Y -\E Y\|^2 \le \E \| Y\|^2
\end{equation}
\begin{align*}
\E \|Y -\E(Y)\|^2 &= \E \sum_i (Y_i - \E(Y_i))^2 \\
&= \sum_i \E (Y_i - \E(Y_i))^2  = \sum_i \E(Y_i^2) -\E (Y_i)^2\\
&  \le  \sum_i \E(Y_i^2) = \E \sum_i Y_i^2 \\
& = \E \|Y \|^2
\end{align*}
\end{itemize}

\end{document}


